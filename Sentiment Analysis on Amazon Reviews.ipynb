{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import bz2 \n\ntrain_file = bz2.BZ2File('/kaggle/input/amazonreviews/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('/kaggle/input/amazonreviews/test.ft.txt.bz2')\n\ntrain_file = train_file.readlines()\ntest_file = test_file.readlines()","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm \n\nnum_train = 800000  # We're training on the first 800,000 reviews in the dataset\nnum_test = 200000  # Using 200,000 reviews from test set\n\ntrain_file = [x.decode('utf-8') for x in tqdm(train_file[:num_train])]\ntest_file = [x.decode('utf-8') for x in tqdm(test_file[:num_test])]\n","execution_count":2,"outputs":[{"output_type":"stream","text":"100%|██████████| 800000/800000 [00:01<00:00, 790444.31it/s]\n100%|██████████| 200000/200000 [00:00<00:00, 815576.10it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_file[0])\nprint(test_file[0])","execution_count":3,"outputs":[{"output_type":"stream","text":"__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n\n__label__2 Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = [0 if x.split()[0] == \"__label__1\" else 1 for x in train_file]\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file] # Get the sequences without upper case and '\\n'\n\ntest_labels = [0 if x.split()[0] == \"__label__1\" else 1 for x in test_file]\ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re \n\n# convert all intergers to zero \nfor i in tqdm(range(len(train_sentences))):\n    train_sentences[i] = re.sub('\\d', '0', train_sentences[i])\n\nfor i in tqdm(range(len(test_sentences))):\n    test_sentences[i] = re.sub('\\d', '0', test_sentences[i])\n    \nfor i in tqdm(range(len(train_sentences))):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com ' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\nfor i in tqdm(range(len(test_sentences))):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com ' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","execution_count":5,"outputs":[{"output_type":"stream","text":"100%|██████████| 800000/800000 [00:11<00:00, 71066.72it/s]\n100%|██████████| 200000/200000 [00:02<00:00, 69753.43it/s]\n100%|██████████| 800000/800000 [00:02<00:00, 329473.18it/s]\n100%|██████████| 200000/200000 [00:00<00:00, 321553.77it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_sentences[0])\nprint(test_sentences[0])","execution_count":6,"outputs":[{"output_type":"stream","text":"stuning even for the non-gamer: this sound track was beautiful! it paints the senery in your mind so well i would recomend it even to people who hate vid. game music! i have played the game chrono cross but out of all of the games i have ever played it has the best music! it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. it would impress anyone who cares to listen! ^_^\ngreat cd: my lovely pat has one of the great voices of her generation. i have listened to this cd for years and i still love it. when i'm in a good mood it makes me feel better. a bad mood just evaporates like sugar in the rain. this cd just oozes life. vocals are jusat stuunning and lyrics just kill. one of life's hidden gems. this is a desert isle cd in my book. why she never made it big is just beyond me. everytime i play this, no matter black, white, young, old, male, female everybody says one thing \"who was that singing ?\"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport nltk \n\nwords = Counter()\nfor i, sentence in enumerate(train_sentences):\n    # The sentences will be stored as a list of words/tokens\n    train_sentences[i] = []\n    for word in nltk.word_tokenize(sentence):\n        words.update([word])\n        train_sentences[i].append(word)\n    if i % 40000 == 0:\n        print(\"Finished : %5.2f %%\" %(i*100/num_train))\nprint(\"Done!\")","execution_count":7,"outputs":[{"output_type":"stream","text":"Finished :  0.00 %\nFinished :  5.00 %\nFinished : 10.00 %\nFinished : 15.00 %\nFinished : 20.00 %\nFinished : 25.00 %\nFinished : 30.00 %\nFinished : 35.00 %\nFinished : 40.00 %\nFinished : 45.00 %\nFinished : 50.00 %\nFinished : 55.00 %\nFinished : 60.00 %\nFinished : 65.00 %\nFinished : 70.00 %\nFinished : 75.00 %\nFinished : 80.00 %\nFinished : 85.00 %\nFinished : 90.00 %\nFinished : 95.00 %\nDone!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the words that only appear once \nwords = {k : v for k,v in words.items() if v>1}\nwords = sorted(words, key=words.get, reverse=True)\n# Adding padding and unknown to our vocabulary so that they will be assigned an index\nwords = ['_PAD', '_UNK'] + words\n# Dictionaries to store the word to index mappings and vice versa\nword2idx = {w : i for i,w in enumerate(words)}\nidx2word = {i : w for i,w in enumerate(words)}","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(word2idx))","execution_count":9,"outputs":[{"output_type":"stream","text":"226162\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, sentence in enumerate(train_sentences):\n    # Looking up the mapping dictionary and assigning the index to the respective words\n    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n\nfor i, sentence in enumerate(test_sentences):\n    # For test sentences, we have to tokenize the sentences as well\n    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \n\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len), dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\nseq_len = 200 \n\ntrain_sentences = pad_input(train_sentences, seq_len)\ntest_sentences = pad_input(test_sentences, seq_len)\n\n# Converting our labels into numpy arrays \ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_frac = 0.2 # 20% validation \nsplit_id = int(split_frac*len(train_labels))\nval_sentences, train_sentences = train_sentences[:split_id], train_sentences[split_id:]\nval_labels , train_labels = train_labels[:split_id], train_labels[split_id:]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch \nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn \n\ntrain_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\nval_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\ntest_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n\nbatch_size = 400 \n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nval_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentNet(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        super(SentimentNet, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        out = self.sigmoid(out)\n        \n        out = out.view(batch_size, -1)\n        out = out[:,-1]\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        return hidden\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(word2idx) + 1 \noutput_size = 1 \nembedding_dim = 400\nhidden_dim = 512\nn_layers = 2 \n\nmodel = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers).to(device)\nlr = 0.005\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 2 \ncounter = 0 \nprint_every = 500\nclip = 5\nval_loss_min = np.Inf\n\nmodel.train()\nfor epoch in range(epochs):\n    h = model.init_hidden(batch_size)\n    for inputs, labels in train_loader:\n        counter += 1\n        model.zero_grad()\n        inputs, labels = inputs.to(device), labels.to(device)\n        h = tuple([e.data for e in h])\n        out, h = model(inputs, h)\n        loss = criterion(out.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        if counter % print_every == 0:\n            model.eval()\n            val_losses = []\n            val_h = model.init_hidden(batch_size)\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                val_h = tuple([e.data for e in val_h])\n                out, val_h = model(inputs, val_h)\n                val_loss = criterion(out.squeeze(), labels.float())\n                val_losses.append(val_loss.item())\n\n            print(\"\\nEPOCH %d/%d... \\t Step: %d... \\t Training Loss: %.4f... \\t Validation Loss: %.4f...\" %(epoch+1, epochs,\n                                                                                      counter, loss.item(), np.mean(val_losses)\n                                                                                     ))\n            if np.mean(val_losses) < val_loss_min:\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min,np.mean(val_losses)))\n                torch.save(model.state_dict(), 'model.pt')\n                val_loss_min = np.mean(val_losses)\n\n        model.train()\n        ","execution_count":17,"outputs":[{"output_type":"stream","text":"\nEPOCH 1/2... \t Step: 500... \t Training Loss: 0.1965... \t Validation Loss: 0.2072...\nValidation loss decreased (inf --> 0.207180).  Saving model ...\n\nEPOCH 1/2... \t Step: 1000... \t Training Loss: 0.1319... \t Validation Loss: 0.1864...\nValidation loss decreased (0.207180 --> 0.186375).  Saving model ...\n\nEPOCH 1/2... \t Step: 1500... \t Training Loss: 0.1976... \t Validation Loss: 0.1776...\nValidation loss decreased (0.186375 --> 0.177598).  Saving model ...\n\nEPOCH 2/2... \t Step: 2000... \t Training Loss: 0.1417... \t Validation Loss: 0.1854...\n\nEPOCH 2/2... \t Step: 2500... \t Training Loss: 0.1959... \t Validation Loss: 0.1774...\nValidation loss decreased (0.177598 --> 0.177352).  Saving model ...\n\nEPOCH 2/2... \t Step: 3000... \t Training Loss: 0.1749... \t Validation Loss: 0.1778...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the best model\nmodel.load_state_dict(torch.load('/kaggle/working/model.pt'))\n\ntest_losses = []\nnum_correct = 0\nh = model.init_hidden(batch_size)\n\nmodel.eval()\nfor inputs, labels in test_loader:\n    h = tuple([each.data for each in h])\n    inputs, labels = inputs.to(device), labels.to(device)\n    output, h = model(inputs, h)\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\ntest_acc = num_correct/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}%\".format(test_acc*100))","execution_count":18,"outputs":[{"output_type":"stream","text":"Test loss: 0.179\nTest accuracy: 93.141%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ok\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}