{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character_level_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ang1p8ZmKuyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_doc(filename):\n",
        "  with open(filename, 'r') as f:\n",
        "    doc = f.read()\n",
        "  f.close()\n",
        "  return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9hPiSOiLMwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "90d0c882-c28a-4ee8-e8ce-4ef1a0efb498"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/data/anna.txt\n",
        "filename = './anna.txt'\n",
        "doc = load_doc(filename)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-09 05:49:22--  https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/recurrent-neural-networks/char-rnn/data/anna.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2025486 (1.9M) [text/plain]\n",
            "Saving to: ‘anna.txt’\n",
            "\n",
            "\ranna.txt              0%[                    ]       0  --.-KB/s               \ranna.txt            100%[===================>]   1.93M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-04-09 05:49:22 (21.9 MB/s) - ‘anna.txt’ saved [2025486/2025486]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JujHKdRGLwOr",
        "colab_type": "code",
        "outputId": "ffed0d7c-f2be-4b1c-ad7b-4c8143081cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "chars = tuple(set(doc))\n",
        "char2int = {ch : i for i, ch in enumerate(chars)}\n",
        "int2char = {i : ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in doc])\n",
        "encoded[:100]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([32, 16, 73, 68, 75, 54, 35, 81, 17, 36, 36, 36, 29, 73, 68, 68, 41,\n",
              "       81, 82, 73, 64, 70, 46, 70, 54, 55, 81, 73, 35, 54, 81, 73, 46, 46,\n",
              "       81, 73, 46, 70, 59, 54, 11, 81, 54, 30, 54, 35, 41, 81,  4, 60, 16,\n",
              "       73, 68, 68, 41, 81, 82, 73, 64, 70, 46, 41, 81, 70, 55, 81,  4, 60,\n",
              "       16, 73, 68, 68, 41, 81, 70, 60, 81, 70, 75, 55, 81, 33, 76, 60, 36,\n",
              "       76, 73, 41,  0, 36, 36, 23, 30, 54, 35, 41, 75, 16, 70, 60])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhgFfFofMRnB",
        "colab_type": "code",
        "outputId": "de12816a-4a33-4ef6-f92f-b6a3a9fae374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "  one_hot = np.zeros((arr.size, n_labels), dtype='float32')\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1 \n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "  return one_hot \n",
        "\n",
        "test_seq = np.array([[4, 5, 2], [1, 3, 0]])\n",
        "one_hot = one_hot_encode(test_seq, 6)\n",
        "print(one_hot)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 1. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve30cWWMQcBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "\n",
        "  total_length = batch_size*seq_length\n",
        "  n_batches = len(arr)//total_length\n",
        "  arr = arr[:n_batches*total_length]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "    x = arr[:, n:n+seq_length]\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "    except IndexError:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
        "    yield x, y \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDwrEuCruVKX",
        "colab_type": "code",
        "outputId": "e34f34fc-b92a-4465-98e3-1e1f8b3e209e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "dataiter = iter(batches)\n",
        "x, y = next(dataiter)\n",
        "print(x[:10, :10])\n",
        "print(y[:10, :10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[32 16 73 68 75 54 35 81 17 36]\n",
            " [55 33 60 81 75 16 73 75 81 73]\n",
            " [54 60 77 81 33 35 81 73 81 82]\n",
            " [55 81 75 16 54 81 13 16 70 54]\n",
            " [81 55 73 76 81 16 54 35 81 75]\n",
            " [13  4 55 55 70 33 60 81 73 60]\n",
            " [81 65 60 60 73 81 16 73 77 81]\n",
            " [ 8 50 46 33 60 55 59 41  0 81]]\n",
            "[[16 73 68 75 54 35 81 17 36 36]\n",
            " [33 60 81 75 16 73 75 81 73 75]\n",
            " [60 77 81 33 35 81 73 81 82 33]\n",
            " [81 75 16 54 81 13 16 70 54 82]\n",
            " [55 73 76 81 16 54 35 81 75 54]\n",
            " [ 4 55 55 70 33 60 81 73 60 77]\n",
            " [65 60 60 73 81 16 73 77 81 55]\n",
            " [50 46 33 60 55 59 41  0 81 61]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zWKgHw3z63v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import torch \n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden, n_layers, drop_prob=0.5, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers \n",
        "    self.n_hidden = n_hidden \n",
        "    self.lr = lr \n",
        "\n",
        "    self.chars = tokens \n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch : i for i,ch in enumerate(self.chars)}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "\n",
        "    # Get the outputs and the new hidden state from the lstm \n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "    # pass through a dropout layer\n",
        "    out = self.dropout(r_output)\n",
        "\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "    out = self.fc(out)\n",
        "    \n",
        "    return out, hidden \n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device), \n",
        "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    \n",
        "    return hidden \n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObQHacYBDi5m",
        "colab_type": "code",
        "outputId": "78e55003-039d-4542-92e1-b2d151764cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "n_hidden = 512\n",
        "n_layers = 2 \n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers).to(device)\n",
        "print(net)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGU7MbRlISuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim \n",
        "\n",
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "  \"\"\"\n",
        "    Argument\n",
        "    net : CharRNN network\n",
        "    data : text data to train the network \n",
        "    epochs : number of epochs to train \n",
        "    batch_size : number of mini-sequences per mini-batch, aka batch size \n",
        "    seq_length : number of character steps per mini-batch \n",
        "    lr: learning rate\n",
        "    clip: gradient clipping\n",
        "    val_frac: Fraction of data to hold out for validation\n",
        "    print_every: Number of steps for printing training and validation loss\n",
        "  \"\"\"\n",
        "  net.train()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "  val_idx = int(val_frac*len(data))\n",
        "  train, val_data = data[:-val_idx], data[-val_idx:]\n",
        "\n",
        "  n_chars = len(net.chars)\n",
        "\n",
        "  for e in range(epochs):\n",
        "\n",
        "    count = 0\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    for x, y in get_batches(train, batch_size, seq_length):\n",
        "\n",
        "      count += 1\n",
        "      x = one_hot_encode(x, n_chars)\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      net.zero_grad()\n",
        "      h = tuple([each.data for each in h])\n",
        "      out, h = net(inputs, h)\n",
        "      loss = criterion(out, targets.view(batch_size*seq_length).long())\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "      optimizer.step()\n",
        "      if count % print_every == 0:\n",
        "        net.eval()\n",
        "        val_losses = []\n",
        "        val_h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "\n",
        "          x = one_hot_encode(x, n_chars)\n",
        "          inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          val_h = tuple([each.data for each in val_h])\n",
        "          out, val_h = net(inputs, val_h)\n",
        "          val_loss = criterion(out, targets.view(batch_size*seq_length).long())\n",
        "          val_losses.append(val_loss.item())\n",
        "        \n",
        "        net.train()\n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "              \"Step: {}...\".format(count),\n",
        "              \"Loss: {:.4f}...\".format(loss.item()),\n",
        "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYNYezsZKuVC",
        "colab_type": "code",
        "outputId": "e0859b07-f266-4e0e-f8c5-bafe9b0b43bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20\n",
        "\n",
        "train(net, encoded, n_epochs, batch_size, seq_length)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2673... Val Loss: 3.2135\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1596... Val Loss: 3.1416\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1459... Val Loss: 3.1263\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1179... Val Loss: 3.1199\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1406... Val Loss: 3.1176\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1177... Val Loss: 3.1152\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1049... Val Loss: 3.1124\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1161... Val Loss: 3.1041\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1058... Val Loss: 3.0839\n",
            "Epoch: 1/20... Step: 100... Loss: 3.0498... Val Loss: 3.0330\n",
            "Epoch: 1/20... Step: 110... Loss: 3.0094... Val Loss: 2.9809\n",
            "Epoch: 1/20... Step: 120... Loss: 2.8584... Val Loss: 2.8388\n",
            "Epoch: 1/20... Step: 130... Loss: 2.7776... Val Loss: 2.7417\n",
            "Epoch: 2/20... Step: 10... Loss: 2.5869... Val Loss: 2.5514\n",
            "Epoch: 2/20... Step: 20... Loss: 2.5119... Val Loss: 2.4936\n",
            "Epoch: 2/20... Step: 30... Loss: 2.5046... Val Loss: 2.4494\n",
            "Epoch: 2/20... Step: 40... Loss: 2.4273... Val Loss: 2.4117\n",
            "Epoch: 2/20... Step: 50... Loss: 2.4477... Val Loss: 2.3831\n",
            "Epoch: 2/20... Step: 60... Loss: 2.3667... Val Loss: 2.3496\n",
            "Epoch: 2/20... Step: 70... Loss: 2.3586... Val Loss: 2.3360\n",
            "Epoch: 2/20... Step: 80... Loss: 2.3218... Val Loss: 2.3021\n",
            "Epoch: 2/20... Step: 90... Loss: 2.3162... Val Loss: 2.2697\n",
            "Epoch: 2/20... Step: 100... Loss: 2.2730... Val Loss: 2.2453\n",
            "Epoch: 2/20... Step: 110... Loss: 2.2327... Val Loss: 2.2193\n",
            "Epoch: 2/20... Step: 120... Loss: 2.1802... Val Loss: 2.1893\n",
            "Epoch: 2/20... Step: 130... Loss: 2.2053... Val Loss: 2.1656\n",
            "Epoch: 3/20... Step: 10... Loss: 2.1499... Val Loss: 2.1219\n",
            "Epoch: 3/20... Step: 20... Loss: 2.1314... Val Loss: 2.0997\n",
            "Epoch: 3/20... Step: 30... Loss: 2.1219... Val Loss: 2.0807\n",
            "Epoch: 3/20... Step: 40... Loss: 2.0756... Val Loss: 2.0602\n",
            "Epoch: 3/20... Step: 50... Loss: 2.1094... Val Loss: 2.0446\n",
            "Epoch: 3/20... Step: 60... Loss: 2.0350... Val Loss: 2.0296\n",
            "Epoch: 3/20... Step: 70... Loss: 2.0315... Val Loss: 2.0122\n",
            "Epoch: 3/20... Step: 80... Loss: 2.0041... Val Loss: 1.9916\n",
            "Epoch: 3/20... Step: 90... Loss: 2.0150... Val Loss: 1.9729\n",
            "Epoch: 3/20... Step: 100... Loss: 1.9723... Val Loss: 1.9581\n",
            "Epoch: 3/20... Step: 110... Loss: 1.9513... Val Loss: 1.9399\n",
            "Epoch: 3/20... Step: 120... Loss: 1.9244... Val Loss: 1.9228\n",
            "Epoch: 3/20... Step: 130... Loss: 1.9540... Val Loss: 1.9076\n",
            "Epoch: 4/20... Step: 10... Loss: 1.9251... Val Loss: 1.8814\n",
            "Epoch: 4/20... Step: 20... Loss: 1.9068... Val Loss: 1.8781\n",
            "Epoch: 4/20... Step: 30... Loss: 1.8999... Val Loss: 1.8624\n",
            "Epoch: 4/20... Step: 40... Loss: 1.8672... Val Loss: 1.8471\n",
            "Epoch: 4/20... Step: 50... Loss: 1.8910... Val Loss: 1.8345\n",
            "Epoch: 4/20... Step: 60... Loss: 1.8321... Val Loss: 1.8204\n",
            "Epoch: 4/20... Step: 70... Loss: 1.8285... Val Loss: 1.8105\n",
            "Epoch: 4/20... Step: 80... Loss: 1.8019... Val Loss: 1.7995\n",
            "Epoch: 4/20... Step: 90... Loss: 1.8229... Val Loss: 1.7867\n",
            "Epoch: 4/20... Step: 100... Loss: 1.7999... Val Loss: 1.7719\n",
            "Epoch: 4/20... Step: 110... Loss: 1.7745... Val Loss: 1.7663\n",
            "Epoch: 4/20... Step: 120... Loss: 1.7496... Val Loss: 1.7607\n",
            "Epoch: 4/20... Step: 130... Loss: 1.7923... Val Loss: 1.7472\n",
            "Epoch: 5/20... Step: 10... Loss: 1.7658... Val Loss: 1.7295\n",
            "Epoch: 5/20... Step: 20... Loss: 1.7702... Val Loss: 1.7241\n",
            "Epoch: 5/20... Step: 30... Loss: 1.7616... Val Loss: 1.7129\n",
            "Epoch: 5/20... Step: 40... Loss: 1.7213... Val Loss: 1.7041\n",
            "Epoch: 5/20... Step: 50... Loss: 1.7531... Val Loss: 1.6978\n",
            "Epoch: 5/20... Step: 60... Loss: 1.6857... Val Loss: 1.6905\n",
            "Epoch: 5/20... Step: 70... Loss: 1.6958... Val Loss: 1.6831\n",
            "Epoch: 5/20... Step: 80... Loss: 1.6867... Val Loss: 1.6747\n",
            "Epoch: 5/20... Step: 90... Loss: 1.6982... Val Loss: 1.6684\n",
            "Epoch: 5/20... Step: 100... Loss: 1.6776... Val Loss: 1.6601\n",
            "Epoch: 5/20... Step: 110... Loss: 1.6544... Val Loss: 1.6584\n",
            "Epoch: 5/20... Step: 120... Loss: 1.6346... Val Loss: 1.6452\n",
            "Epoch: 5/20... Step: 130... Loss: 1.6852... Val Loss: 1.6474\n",
            "Epoch: 6/20... Step: 10... Loss: 1.6639... Val Loss: 1.6275\n",
            "Epoch: 6/20... Step: 20... Loss: 1.6775... Val Loss: 1.6208\n",
            "Epoch: 6/20... Step: 30... Loss: 1.6596... Val Loss: 1.6167\n",
            "Epoch: 6/20... Step: 40... Loss: 1.6270... Val Loss: 1.6104\n",
            "Epoch: 6/20... Step: 50... Loss: 1.6547... Val Loss: 1.6086\n",
            "Epoch: 6/20... Step: 60... Loss: 1.5957... Val Loss: 1.5983\n",
            "Epoch: 6/20... Step: 70... Loss: 1.6062... Val Loss: 1.5984\n",
            "Epoch: 6/20... Step: 80... Loss: 1.5939... Val Loss: 1.5926\n",
            "Epoch: 6/20... Step: 90... Loss: 1.6111... Val Loss: 1.5851\n",
            "Epoch: 6/20... Step: 100... Loss: 1.5943... Val Loss: 1.5805\n",
            "Epoch: 6/20... Step: 110... Loss: 1.5790... Val Loss: 1.5773\n",
            "Epoch: 6/20... Step: 120... Loss: 1.5632... Val Loss: 1.5801\n",
            "Epoch: 6/20... Step: 130... Loss: 1.5948... Val Loss: 1.5688\n",
            "Epoch: 7/20... Step: 10... Loss: 1.6025... Val Loss: 1.5547\n",
            "Epoch: 7/20... Step: 20... Loss: 1.6076... Val Loss: 1.5493\n",
            "Epoch: 7/20... Step: 30... Loss: 1.5880... Val Loss: 1.5476\n",
            "Epoch: 7/20... Step: 40... Loss: 1.5547... Val Loss: 1.5447\n",
            "Epoch: 7/20... Step: 50... Loss: 1.5837... Val Loss: 1.5406\n",
            "Epoch: 7/20... Step: 60... Loss: 1.5102... Val Loss: 1.5346\n",
            "Epoch: 7/20... Step: 70... Loss: 1.5386... Val Loss: 1.5330\n",
            "Epoch: 7/20... Step: 80... Loss: 1.5274... Val Loss: 1.5295\n",
            "Epoch: 7/20... Step: 90... Loss: 1.5476... Val Loss: 1.5243\n",
            "Epoch: 7/20... Step: 100... Loss: 1.5261... Val Loss: 1.5210\n",
            "Epoch: 7/20... Step: 110... Loss: 1.5060... Val Loss: 1.5236\n",
            "Epoch: 7/20... Step: 120... Loss: 1.4993... Val Loss: 1.5136\n",
            "Epoch: 7/20... Step: 130... Loss: 1.5403... Val Loss: 1.5104\n",
            "Epoch: 8/20... Step: 10... Loss: 1.5475... Val Loss: 1.5102\n",
            "Epoch: 8/20... Step: 20... Loss: 1.5580... Val Loss: 1.5007\n",
            "Epoch: 8/20... Step: 30... Loss: 1.5280... Val Loss: 1.5007\n",
            "Epoch: 8/20... Step: 40... Loss: 1.5028... Val Loss: 1.4955\n",
            "Epoch: 8/20... Step: 50... Loss: 1.5325... Val Loss: 1.4926\n",
            "Epoch: 8/20... Step: 60... Loss: 1.4588... Val Loss: 1.4890\n",
            "Epoch: 8/20... Step: 70... Loss: 1.4726... Val Loss: 1.4866\n",
            "Epoch: 8/20... Step: 80... Loss: 1.4700... Val Loss: 1.4853\n",
            "Epoch: 8/20... Step: 90... Loss: 1.4860... Val Loss: 1.4792\n",
            "Epoch: 8/20... Step: 100... Loss: 1.4790... Val Loss: 1.4757\n",
            "Epoch: 8/20... Step: 110... Loss: 1.4558... Val Loss: 1.4799\n",
            "Epoch: 8/20... Step: 120... Loss: 1.4433... Val Loss: 1.4707\n",
            "Epoch: 8/20... Step: 130... Loss: 1.4851... Val Loss: 1.4657\n",
            "Epoch: 9/20... Step: 10... Loss: 1.4963... Val Loss: 1.4615\n",
            "Epoch: 9/20... Step: 20... Loss: 1.5152... Val Loss: 1.4597\n",
            "Epoch: 9/20... Step: 30... Loss: 1.4908... Val Loss: 1.4566\n",
            "Epoch: 9/20... Step: 40... Loss: 1.4520... Val Loss: 1.4562\n",
            "Epoch: 9/20... Step: 50... Loss: 1.4833... Val Loss: 1.4526\n",
            "Epoch: 9/20... Step: 60... Loss: 1.4171... Val Loss: 1.4504\n",
            "Epoch: 9/20... Step: 70... Loss: 1.4323... Val Loss: 1.4489\n",
            "Epoch: 9/20... Step: 80... Loss: 1.4347... Val Loss: 1.4535\n",
            "Epoch: 9/20... Step: 90... Loss: 1.4387... Val Loss: 1.4432\n",
            "Epoch: 9/20... Step: 100... Loss: 1.4318... Val Loss: 1.4416\n",
            "Epoch: 9/20... Step: 110... Loss: 1.4215... Val Loss: 1.4412\n",
            "Epoch: 9/20... Step: 120... Loss: 1.4037... Val Loss: 1.4370\n",
            "Epoch: 9/20... Step: 130... Loss: 1.4431... Val Loss: 1.4329\n",
            "Epoch: 10/20... Step: 10... Loss: 1.4655... Val Loss: 1.4281\n",
            "Epoch: 10/20... Step: 20... Loss: 1.4700... Val Loss: 1.4273\n",
            "Epoch: 10/20... Step: 30... Loss: 1.4531... Val Loss: 1.4219\n",
            "Epoch: 10/20... Step: 40... Loss: 1.4243... Val Loss: 1.4246\n",
            "Epoch: 10/20... Step: 50... Loss: 1.4469... Val Loss: 1.4230\n",
            "Epoch: 10/20... Step: 60... Loss: 1.3774... Val Loss: 1.4221\n",
            "Epoch: 10/20... Step: 70... Loss: 1.4020... Val Loss: 1.4168\n",
            "Epoch: 10/20... Step: 80... Loss: 1.3930... Val Loss: 1.4175\n",
            "Epoch: 10/20... Step: 90... Loss: 1.4128... Val Loss: 1.4105\n",
            "Epoch: 10/20... Step: 100... Loss: 1.4098... Val Loss: 1.4108\n",
            "Epoch: 10/20... Step: 110... Loss: 1.3782... Val Loss: 1.4113\n",
            "Epoch: 10/20... Step: 120... Loss: 1.3732... Val Loss: 1.4101\n",
            "Epoch: 10/20... Step: 130... Loss: 1.4098... Val Loss: 1.4077\n",
            "Epoch: 11/20... Step: 10... Loss: 1.4346... Val Loss: 1.4073\n",
            "Epoch: 11/20... Step: 20... Loss: 1.4362... Val Loss: 1.3999\n",
            "Epoch: 11/20... Step: 30... Loss: 1.4230... Val Loss: 1.3949\n",
            "Epoch: 11/20... Step: 40... Loss: 1.3923... Val Loss: 1.3975\n",
            "Epoch: 11/20... Step: 50... Loss: 1.4157... Val Loss: 1.3921\n",
            "Epoch: 11/20... Step: 60... Loss: 1.3452... Val Loss: 1.3947\n",
            "Epoch: 11/20... Step: 70... Loss: 1.3735... Val Loss: 1.3915\n",
            "Epoch: 11/20... Step: 80... Loss: 1.3663... Val Loss: 1.3940\n",
            "Epoch: 11/20... Step: 90... Loss: 1.3715... Val Loss: 1.3853\n",
            "Epoch: 11/20... Step: 100... Loss: 1.3675... Val Loss: 1.3859\n",
            "Epoch: 11/20... Step: 110... Loss: 1.3567... Val Loss: 1.3842\n",
            "Epoch: 11/20... Step: 120... Loss: 1.3401... Val Loss: 1.3853\n",
            "Epoch: 11/20... Step: 130... Loss: 1.3720... Val Loss: 1.3813\n",
            "Epoch: 12/20... Step: 10... Loss: 1.3977... Val Loss: 1.3773\n",
            "Epoch: 12/20... Step: 20... Loss: 1.4043... Val Loss: 1.3744\n",
            "Epoch: 12/20... Step: 30... Loss: 1.3877... Val Loss: 1.3693\n",
            "Epoch: 12/20... Step: 40... Loss: 1.3533... Val Loss: 1.3729\n",
            "Epoch: 12/20... Step: 50... Loss: 1.3818... Val Loss: 1.3734\n",
            "Epoch: 12/20... Step: 60... Loss: 1.3186... Val Loss: 1.3728\n",
            "Epoch: 12/20... Step: 70... Loss: 1.3349... Val Loss: 1.3671\n",
            "Epoch: 12/20... Step: 80... Loss: 1.3293... Val Loss: 1.3646\n",
            "Epoch: 12/20... Step: 90... Loss: 1.3459... Val Loss: 1.3661\n",
            "Epoch: 12/20... Step: 100... Loss: 1.3447... Val Loss: 1.3599\n",
            "Epoch: 12/20... Step: 110... Loss: 1.3280... Val Loss: 1.3622\n",
            "Epoch: 12/20... Step: 120... Loss: 1.3073... Val Loss: 1.3653\n",
            "Epoch: 12/20... Step: 130... Loss: 1.3451... Val Loss: 1.3561\n",
            "Epoch: 13/20... Step: 10... Loss: 1.3741... Val Loss: 1.3572\n",
            "Epoch: 13/20... Step: 20... Loss: 1.3848... Val Loss: 1.3545\n",
            "Epoch: 13/20... Step: 30... Loss: 1.3653... Val Loss: 1.3471\n",
            "Epoch: 13/20... Step: 40... Loss: 1.3233... Val Loss: 1.3496\n",
            "Epoch: 13/20... Step: 50... Loss: 1.3537... Val Loss: 1.3487\n",
            "Epoch: 13/20... Step: 60... Loss: 1.2850... Val Loss: 1.3481\n",
            "Epoch: 13/20... Step: 70... Loss: 1.3152... Val Loss: 1.3500\n",
            "Epoch: 13/20... Step: 80... Loss: 1.3052... Val Loss: 1.3463\n",
            "Epoch: 13/20... Step: 90... Loss: 1.3212... Val Loss: 1.3446\n",
            "Epoch: 13/20... Step: 100... Loss: 1.3191... Val Loss: 1.3412\n",
            "Epoch: 13/20... Step: 110... Loss: 1.3083... Val Loss: 1.3408\n",
            "Epoch: 13/20... Step: 120... Loss: 1.2855... Val Loss: 1.3432\n",
            "Epoch: 13/20... Step: 130... Loss: 1.3192... Val Loss: 1.3405\n",
            "Epoch: 14/20... Step: 10... Loss: 1.3364... Val Loss: 1.3338\n",
            "Epoch: 14/20... Step: 20... Loss: 1.3525... Val Loss: 1.3327\n",
            "Epoch: 14/20... Step: 30... Loss: 1.3405... Val Loss: 1.3290\n",
            "Epoch: 14/20... Step: 40... Loss: 1.3071... Val Loss: 1.3319\n",
            "Epoch: 14/20... Step: 50... Loss: 1.3299... Val Loss: 1.3320\n",
            "Epoch: 14/20... Step: 60... Loss: 1.2606... Val Loss: 1.3326\n",
            "Epoch: 14/20... Step: 70... Loss: 1.2840... Val Loss: 1.3288\n",
            "Epoch: 14/20... Step: 80... Loss: 1.2746... Val Loss: 1.3240\n",
            "Epoch: 14/20... Step: 90... Loss: 1.3006... Val Loss: 1.3249\n",
            "Epoch: 14/20... Step: 100... Loss: 1.2900... Val Loss: 1.3245\n",
            "Epoch: 14/20... Step: 110... Loss: 1.2796... Val Loss: 1.3234\n",
            "Epoch: 14/20... Step: 120... Loss: 1.2613... Val Loss: 1.3274\n",
            "Epoch: 14/20... Step: 130... Loss: 1.2940... Val Loss: 1.3213\n",
            "Epoch: 15/20... Step: 10... Loss: 1.3308... Val Loss: 1.3269\n",
            "Epoch: 15/20... Step: 20... Loss: 1.3484... Val Loss: 1.3191\n",
            "Epoch: 15/20... Step: 30... Loss: 1.3211... Val Loss: 1.3187\n",
            "Epoch: 15/20... Step: 40... Loss: 1.2848... Val Loss: 1.3190\n",
            "Epoch: 15/20... Step: 50... Loss: 1.3102... Val Loss: 1.3182\n",
            "Epoch: 15/20... Step: 60... Loss: 1.2485... Val Loss: 1.3179\n",
            "Epoch: 15/20... Step: 70... Loss: 1.2780... Val Loss: 1.3171\n",
            "Epoch: 15/20... Step: 80... Loss: 1.2639... Val Loss: 1.3183\n",
            "Epoch: 15/20... Step: 90... Loss: 1.2762... Val Loss: 1.3158\n",
            "Epoch: 15/20... Step: 100... Loss: 1.2698... Val Loss: 1.3140\n",
            "Epoch: 15/20... Step: 110... Loss: 1.2653... Val Loss: 1.3096\n",
            "Epoch: 15/20... Step: 120... Loss: 1.2534... Val Loss: 1.3126\n",
            "Epoch: 15/20... Step: 130... Loss: 1.2770... Val Loss: 1.3094\n",
            "Epoch: 16/20... Step: 10... Loss: 1.2942... Val Loss: 1.3128\n",
            "Epoch: 16/20... Step: 20... Loss: 1.3146... Val Loss: 1.3065\n",
            "Epoch: 16/20... Step: 30... Loss: 1.3014... Val Loss: 1.3041\n",
            "Epoch: 16/20... Step: 40... Loss: 1.2725... Val Loss: 1.3066\n",
            "Epoch: 16/20... Step: 50... Loss: 1.2940... Val Loss: 1.3080\n",
            "Epoch: 16/20... Step: 60... Loss: 1.2326... Val Loss: 1.3066\n",
            "Epoch: 16/20... Step: 70... Loss: 1.2602... Val Loss: 1.3080\n",
            "Epoch: 16/20... Step: 80... Loss: 1.2384... Val Loss: 1.3051\n",
            "Epoch: 16/20... Step: 90... Loss: 1.2634... Val Loss: 1.2997\n",
            "Epoch: 16/20... Step: 100... Loss: 1.2567... Val Loss: 1.2997\n",
            "Epoch: 16/20... Step: 110... Loss: 1.2480... Val Loss: 1.3052\n",
            "Epoch: 16/20... Step: 120... Loss: 1.2361... Val Loss: 1.3013\n",
            "Epoch: 16/20... Step: 130... Loss: 1.2584... Val Loss: 1.3018\n",
            "Epoch: 17/20... Step: 10... Loss: 1.2894... Val Loss: 1.3052\n",
            "Epoch: 17/20... Step: 20... Loss: 1.3061... Val Loss: 1.3004\n",
            "Epoch: 17/20... Step: 30... Loss: 1.2932... Val Loss: 1.2966\n",
            "Epoch: 17/20... Step: 40... Loss: 1.2473... Val Loss: 1.2979\n",
            "Epoch: 17/20... Step: 50... Loss: 1.2764... Val Loss: 1.2997\n",
            "Epoch: 17/20... Step: 60... Loss: 1.2134... Val Loss: 1.3030\n",
            "Epoch: 17/20... Step: 70... Loss: 1.2473... Val Loss: 1.2996\n",
            "Epoch: 17/20... Step: 80... Loss: 1.2308... Val Loss: 1.2973\n",
            "Epoch: 17/20... Step: 90... Loss: 1.2543... Val Loss: 1.2954\n",
            "Epoch: 17/20... Step: 100... Loss: 1.2447... Val Loss: 1.2929\n",
            "Epoch: 17/20... Step: 110... Loss: 1.2330... Val Loss: 1.2919\n",
            "Epoch: 17/20... Step: 120... Loss: 1.2278... Val Loss: 1.2913\n",
            "Epoch: 17/20... Step: 130... Loss: 1.2501... Val Loss: 1.2912\n",
            "Epoch: 18/20... Step: 10... Loss: 1.2763... Val Loss: 1.2960\n",
            "Epoch: 18/20... Step: 20... Loss: 1.2853... Val Loss: 1.2928\n",
            "Epoch: 18/20... Step: 30... Loss: 1.2681... Val Loss: 1.2847\n",
            "Epoch: 18/20... Step: 40... Loss: 1.2506... Val Loss: 1.2870\n",
            "Epoch: 18/20... Step: 50... Loss: 1.2568... Val Loss: 1.2922\n",
            "Epoch: 18/20... Step: 60... Loss: 1.1999... Val Loss: 1.2909\n",
            "Epoch: 18/20... Step: 70... Loss: 1.2298... Val Loss: 1.2877\n",
            "Epoch: 18/20... Step: 80... Loss: 1.2116... Val Loss: 1.2872\n",
            "Epoch: 18/20... Step: 90... Loss: 1.2289... Val Loss: 1.2904\n",
            "Epoch: 18/20... Step: 100... Loss: 1.2261... Val Loss: 1.2849\n",
            "Epoch: 18/20... Step: 110... Loss: 1.2259... Val Loss: 1.2825\n",
            "Epoch: 18/20... Step: 120... Loss: 1.2088... Val Loss: 1.2889\n",
            "Epoch: 18/20... Step: 130... Loss: 1.2327... Val Loss: 1.2870\n",
            "Epoch: 19/20... Step: 10... Loss: 1.2566... Val Loss: 1.2898\n",
            "Epoch: 19/20... Step: 20... Loss: 1.2723... Val Loss: 1.2865\n",
            "Epoch: 19/20... Step: 30... Loss: 1.2689... Val Loss: 1.2820\n",
            "Epoch: 19/20... Step: 40... Loss: 1.2341... Val Loss: 1.2791\n",
            "Epoch: 19/20... Step: 50... Loss: 1.2524... Val Loss: 1.2844\n",
            "Epoch: 19/20... Step: 60... Loss: 1.1878... Val Loss: 1.2860\n",
            "Epoch: 19/20... Step: 70... Loss: 1.2070... Val Loss: 1.2852\n",
            "Epoch: 19/20... Step: 80... Loss: 1.2070... Val Loss: 1.2850\n",
            "Epoch: 19/20... Step: 90... Loss: 1.2234... Val Loss: 1.2809\n",
            "Epoch: 19/20... Step: 100... Loss: 1.2182... Val Loss: 1.2847\n",
            "Epoch: 19/20... Step: 110... Loss: 1.2074... Val Loss: 1.2838\n",
            "Epoch: 19/20... Step: 120... Loss: 1.1965... Val Loss: 1.2830\n",
            "Epoch: 19/20... Step: 130... Loss: 1.2194... Val Loss: 1.2802\n",
            "Epoch: 20/20... Step: 10... Loss: 1.2434... Val Loss: 1.2847\n",
            "Epoch: 20/20... Step: 20... Loss: 1.2654... Val Loss: 1.2795\n",
            "Epoch: 20/20... Step: 30... Loss: 1.2513... Val Loss: 1.2778\n",
            "Epoch: 20/20... Step: 40... Loss: 1.2185... Val Loss: 1.2789\n",
            "Epoch: 20/20... Step: 50... Loss: 1.2394... Val Loss: 1.2786\n",
            "Epoch: 20/20... Step: 60... Loss: 1.1785... Val Loss: 1.2801\n",
            "Epoch: 20/20... Step: 70... Loss: 1.2053... Val Loss: 1.2795\n",
            "Epoch: 20/20... Step: 80... Loss: 1.1956... Val Loss: 1.2778\n",
            "Epoch: 20/20... Step: 90... Loss: 1.2085... Val Loss: 1.2772\n",
            "Epoch: 20/20... Step: 100... Loss: 1.2042... Val Loss: 1.2788\n",
            "Epoch: 20/20... Step: 110... Loss: 1.1983... Val Loss: 1.2753\n",
            "Epoch: 20/20... Step: 120... Loss: 1.1820... Val Loss: 1.2722\n",
            "Epoch: 20/20... Step: 130... Loss: 1.2001... Val Loss: 1.2713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUFCNO15XFFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'rnn_x_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden' : net.n_hidden,\n",
        "              'n_layers' : net.n_layers,\n",
        "              'state_dict' : net.state_dict(),\n",
        "              'tokens' : net.chars}\n",
        "with open(model_name, 'wb') as f:\n",
        "  torch.save(checkpoint, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-CqG9shPg7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "\n",
        "  x = np.array([[net.char2int[char]]])\n",
        "  x = one_hot_encode(x, len(net.chars))\n",
        "  inputs = torch.from_numpy(x).to(device)\n",
        "  h = tuple([each.data for each in h])\n",
        "  out, h = net(inputs, h)\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  p = p.to(\"cpu\")\n",
        "\n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(net.chars))\n",
        "  else:\n",
        "    p, top_ch = p.topk(top_k)\n",
        "    top_ch = top_ch.numpy().squeeze()\n",
        "  \n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "  return net.int2char[char], h "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3tkQbkCbCX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "  net.to(device)\n",
        "  net.eval()\n",
        "  chars = [ch for ch in prime]\n",
        "  h = net.init_hidden(1)\n",
        "  for ch in prime:\n",
        "    char, h = predict(net, ch, h, top_k=top_k)\n",
        "  \n",
        "  chars.append(char)\n",
        "\n",
        "  for ii in range(size):\n",
        "    char, h = predict(net, char[-1], h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "  \n",
        "  return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfJZi5VLcPMu",
        "colab_type": "code",
        "outputId": "7f039ce4-49df-47e3-f018-9c1cb12e3653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "print(sample(net, 1000, prime='Anana', top_k=5))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anana had so\n",
            "likely that he was the sound of the sort of angure that she\n",
            "had not been so much too was the setten of that stucider, his fear\n",
            "that hed head. A divorce who had been doubt, but he had tears at once\n",
            "that they done they were to look it. The peasants carreed a such\n",
            "sure with the son in the doctor's frequent on three commots of the\n",
            "same.\n",
            "\n",
            "\"I've not sat the sort in the matter,\" she thought, and shiling the\n",
            "princess as though it were as all the part of these particulation of\n",
            "her hotes, and startly faller up. The sama shalles of this weeled shade of her hat\n",
            "to hird her. That said the face was happiness, which had been to dine\n",
            "in the choldes that she would say that he did not think of them, and so\n",
            "to\n",
            "show me at him. She could stand him, and, said to the princess for his\n",
            "his side, stopping to his belatoo with a cheerful party to show the\n",
            "tempheration.\n",
            "\n",
            "\"You don't know.\"\n",
            "\n",
            "The princess went to the same significance, when Alexey\n",
            "Alexandrovitch had talked of the children to the country, his s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8oprluicfjg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34818ebe-ad2f-4b05-8df2-862769bcc361"
      },
      "source": [
        "with open('rnn_x_epoch.net', 'rb') as f:\n",
        "  checkpoint = torch.load(f)\n",
        "loaded = CharRNN(tokens=checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkLHToDFegyU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "5904560f-269c-4cfd-dbd1-48beeb62bc14"
      },
      "source": [
        "print(sample(net, 1000, top_k=5, prime=\"And Levin said\"))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Levin said,\n",
            "and his fingers the sides were trying somewhere and his hands of\n",
            "the sample of anything that seemed to her, that the old man, he\n",
            "went on, she would have to speak to the story and at the second\n",
            "consideration when this to call her somewish, her house that the\n",
            "more conversation in her son she saw that in the class she went on\n",
            "tabinitly on the country. \"Ah! have you think it all,\" he added, smalighing,\n",
            "but he would have letter alone in all that touching the\n",
            "chicken and at the morning, and was standing and who took him her\n",
            "life as those houses as he saw the stands. He was answered, steppand and\n",
            "looking to a little. His brother was to be had to say.\n",
            "\n",
            "\"What have you seen my sight of her sun those well, I would be\n",
            "decised. There's no terricill moment. Wanter to go and see them. Whon the\n",
            "mother in the such such answer is the pretty satisfressor of the\n",
            "country, but the sound of steps, and seen him, and we see him, and\n",
            "thinking of the politics if it's a man of what we can grass his\n",
            "supper, with \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS-o1dwOAgmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}