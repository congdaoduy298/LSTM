{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_truyenkieu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Die5wDUkIxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax0_c4TVlDUC",
        "colab_type": "code",
        "outputId": "06770f81-6e59-4120-923a-70983f2dc3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('truyenkieu.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "text[:30]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1..Trăm năm trong cõi người ta'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXaupVoflLxZ",
        "colab_type": "code",
        "outputId": "2a10e36a-138c-4eca-c454-51bcd7040518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = re.sub(r'[0-9\\.]', '', text)\n",
        "text[:30]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Trăm năm trong cõi người ta,\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUKlBLGUl2gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = list(set(text))\n",
        "char2int = {c:i for i,c in enumerate(chars)}\n",
        "int2char = {i:c for i,c in enumerate(chars)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vlCwIqzmZA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_text = np.array([char2int[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK0NeNhDx6Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_size = int(0.1*len(int_text))\n",
        "val_int_text, int_text = int_text[:val_size], int_text[val_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1WCxXeamexL",
        "colab_type": "code",
        "outputId": "eae26ca3-c132-4ccb-a5bb-89aa022e7823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(f\"Number chars in text : {len(text)}\")\n",
        "print(f\"Number unique chars in text : {len(chars)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number chars in text : 107561\n",
            "Number unique chars in text : 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIu4a2usmxXA",
        "colab_type": "code",
        "outputId": "d8003e5e-6885-42a3-e207-46dd2f0b4ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "def one_hot_encode(arr, vocab_size):\n",
        "  x = np.zeros((arr.size, vocab_size), np.float32)\n",
        "  x[np.arange(arr.size), arr.flatten()] = 1\n",
        "  x = x.reshape((*arr.shape, vocab_size))\n",
        "  return x \n",
        "\n",
        "arr = np.array([[5, 3, 2],\n",
        "       [1, 1, 1]])\n",
        "\n",
        "print(one_hot_encode(arr, 6))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4AmXCguoMDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batches(arr, batch_size, seq_len):\n",
        "  full_batch = batch_size*seq_len\n",
        "  num_batches = len(arr)//full_batch \n",
        "  arr = arr[:num_batches*full_batch]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "  for i in range(0, arr.shape[1], seq_len):\n",
        "    x = arr[:, i:i+seq_len]\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, i+seq_len+1]\n",
        "    except IndexError:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "    yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIBa73tpp7tg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a2a63af3-04ae-448d-d600-d92f8d3c17cb"
      },
      "source": [
        "x, y = next(iter(generate_batches(int_text, 3, 10)))\n",
        "print(x)\n",
        "print(\"---------------------\")\n",
        "print(y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 56  31  59  50  75  11  92  50  96 101]\n",
            " [ 80  50  10  39  50 126 124 116  31  50]\n",
            " [ 50  42  52  31  59  50  75  76  92  50]]\n",
            "---------------------\n",
            "[[ 31  59  50  75  11  92  50  96 101  42]\n",
            " [ 50  10  39  50 126 124 116  31  50  92]\n",
            " [ 42  52  31  59  50  75  76  92  50  92]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNnRNoHor0Yz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "26e166dd-d858-434c-edd1-876e08a4cda7"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "\n",
        "class Network(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, use_one_hot):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim \n",
        "    self.n_layers = n_layers\n",
        "    self.use_one_hot = use_one_hot\n",
        "    if not use_one_hot:\n",
        "      self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "      self.LSTM = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=0.5, batch_first=True)\n",
        "    else:\n",
        "      self.LSTM = nn.LSTM(vocab_size, hidden_dim, n_layers, dropout=0.5, batch_first=True)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "    self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    if not self.use_one_hot:\n",
        "      x = self.embedding(x)\n",
        "    x, hidden = self.LSTM(x, hidden)\n",
        "    x = x.contiguous().view(-1, self.hidden_dim)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc(x)\n",
        "    return x, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "              weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "    return hidden \n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "vocab_size = len(chars)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "one_hot_model = Network(vocab_size, embedding_dim, hidden_dim, n_layers, use_one_hot=True).to(device)\n",
        "embedding_model = Network(vocab_size, embedding_dim, hidden_dim, n_layers, use_one_hot=False).to(device)\n",
        "print(one_hot_model)\n",
        "print(embedding_model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (LSTM): LSTM(128, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
            ")\n",
            "Network(\n",
            "  (embedding): Embedding(128, 100)\n",
            "  (LSTM): LSTM(100, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtMEyGLeuyYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim \n",
        "\n",
        "def train_model(model, use_one_hot):\n",
        "\n",
        "  if use_one_hot:\n",
        "    print(\"Model use one hot encoded ~~~\")\n",
        "  else:\n",
        "    print(\"Model use embedding layer !!!\")\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  epochs = 30\n",
        "  batch_size = 20\n",
        "  seq_len = 100\n",
        "  clip = 5\n",
        "  min_val_loss = np.Inf\n",
        "\n",
        "  val_idx = int(0.1*len(int_text))\n",
        "  train, val_data = int_text[:-val_idx], int_text[-val_idx:]\n",
        "  model.train()\n",
        "  for e in range(epochs):\n",
        "    h = model.init_hidden(batch_size)\n",
        "    for x, y in generate_batches(train, batch_size, seq_len):\n",
        "      optimizer.zero_grad()\n",
        "      if use_one_hot:\n",
        "        x = one_hot_encode(x, vocab_size)\n",
        "      x, y = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
        "      h = tuple([e.data for e in h])\n",
        "      out, h = model(x, h)\n",
        "      loss = criterion(out, y.view(batch_size*seq_len).long())\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "      optimizer.step()\n",
        "    \n",
        "    else:\n",
        "      model.eval()\n",
        "      val_losses = []\n",
        "      # with torch.no_grad():\n",
        "      val_h = model.init_hidden(batch_size)\n",
        "      for x, y in generate_batches(val_data, batch_size, seq_len):\n",
        "        if use_one_hot:\n",
        "          x = one_hot_encode(x, vocab_size)\n",
        "        x, y = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
        "        val_h = tuple([e.data for e in h])\n",
        "        out, h = model(x, h)\n",
        "        val_loss = criterion(out, y.view(batch_size*seq_len).long())\n",
        "        val_losses.append(val_loss.item())\n",
        "      print(\"Epoch {}/{}.\\tTraining loss : {:6.4f}\\tValidation loss : {:6.4f}\".format(e+1, epochs, loss.item(), np.mean(val_losses)))\n",
        "      if val_loss < min_val_loss:\n",
        "        print(\"Validation loss decrease {:6.4f} ----> {:6.4f}.\".format(min_val_loss, val_loss))\n",
        "        min_val_loss = val_loss\n",
        "        print(\"Save weights...\")\n",
        "        torch.save(model.state_dict(), f'model_{int(use_one_hot)}.h5')\n",
        "      model.train()\n",
        "  print(\"------------------------------------------------\")\n",
        "  return model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQpKfpqONwmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f778b421-2a29-4bc5-bd8d-4b621c8e09a7"
      },
      "source": [
        "one_hot_model = train_model(one_hot_model, use_one_hot=True)\n",
        "embedding_model = train_model(embedding_model, use_one_hot=False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model use one hot encoded ~~~\n",
            "Epoch 1/30.\tTraining loss : 3.5307\tValidation loss : 3.5280\n",
            "Validation loss decrease    inf ----> 3.5320.\n",
            "Save weights...\n",
            "Epoch 2/30.\tTraining loss : 3.4254\tValidation loss : 3.4126\n",
            "Validation loss decrease 3.5320 ----> 3.4144.\n",
            "Save weights...\n",
            "Epoch 3/30.\tTraining loss : 2.7916\tValidation loss : 2.7273\n",
            "Validation loss decrease 3.4144 ----> 2.7283.\n",
            "Save weights...\n",
            "Epoch 4/30.\tTraining loss : 2.4426\tValidation loss : 2.3888\n",
            "Validation loss decrease 2.7283 ----> 2.3916.\n",
            "Save weights...\n",
            "Epoch 5/30.\tTraining loss : 2.2849\tValidation loss : 2.2232\n",
            "Validation loss decrease 2.3916 ----> 2.2177.\n",
            "Save weights...\n",
            "Epoch 6/30.\tTraining loss : 2.1614\tValidation loss : 2.1115\n",
            "Validation loss decrease 2.2177 ----> 2.1007.\n",
            "Save weights...\n",
            "Epoch 7/30.\tTraining loss : 2.0658\tValidation loss : 2.0272\n",
            "Validation loss decrease 2.1007 ----> 2.0157.\n",
            "Save weights...\n",
            "Epoch 8/30.\tTraining loss : 1.9969\tValidation loss : 1.9546\n",
            "Validation loss decrease 2.0157 ----> 1.9393.\n",
            "Save weights...\n",
            "Epoch 9/30.\tTraining loss : 1.9453\tValidation loss : 1.9019\n",
            "Validation loss decrease 1.9393 ----> 1.8845.\n",
            "Save weights...\n",
            "Epoch 10/30.\tTraining loss : 1.9057\tValidation loss : 1.8743\n",
            "Validation loss decrease 1.8845 ----> 1.8531.\n",
            "Save weights...\n",
            "Epoch 11/30.\tTraining loss : 1.8626\tValidation loss : 1.8272\n",
            "Validation loss decrease 1.8531 ----> 1.8078.\n",
            "Save weights...\n",
            "Epoch 12/30.\tTraining loss : 1.8196\tValidation loss : 1.8073\n",
            "Validation loss decrease 1.8078 ----> 1.7838.\n",
            "Save weights...\n",
            "Epoch 13/30.\tTraining loss : 1.7911\tValidation loss : 1.7746\n",
            "Validation loss decrease 1.7838 ----> 1.7469.\n",
            "Save weights...\n",
            "Epoch 14/30.\tTraining loss : 1.7782\tValidation loss : 1.7520\n",
            "Validation loss decrease 1.7469 ----> 1.7324.\n",
            "Save weights...\n",
            "Epoch 15/30.\tTraining loss : 1.7385\tValidation loss : 1.7445\n",
            "Validation loss decrease 1.7324 ----> 1.7144.\n",
            "Save weights...\n",
            "Epoch 16/30.\tTraining loss : 1.7235\tValidation loss : 1.7286\n",
            "Validation loss decrease 1.7144 ----> 1.6968.\n",
            "Save weights...\n",
            "Epoch 17/30.\tTraining loss : 1.7110\tValidation loss : 1.7056\n",
            "Validation loss decrease 1.6968 ----> 1.6777.\n",
            "Save weights...\n",
            "Epoch 18/30.\tTraining loss : 1.6978\tValidation loss : 1.6950\n",
            "Validation loss decrease 1.6777 ----> 1.6670.\n",
            "Save weights...\n",
            "Epoch 19/30.\tTraining loss : 1.6750\tValidation loss : 1.6845\n",
            "Validation loss decrease 1.6670 ----> 1.6596.\n",
            "Save weights...\n",
            "Epoch 20/30.\tTraining loss : 1.6570\tValidation loss : 1.6782\n",
            "Validation loss decrease 1.6596 ----> 1.6502.\n",
            "Save weights...\n",
            "Epoch 21/30.\tTraining loss : 1.6633\tValidation loss : 1.6667\n",
            "Validation loss decrease 1.6502 ----> 1.6393.\n",
            "Save weights...\n",
            "Epoch 22/30.\tTraining loss : 1.6432\tValidation loss : 1.6591\n",
            "Validation loss decrease 1.6393 ----> 1.6349.\n",
            "Save weights...\n",
            "Epoch 23/30.\tTraining loss : 1.6091\tValidation loss : 1.6507\n",
            "Validation loss decrease 1.6349 ----> 1.6298.\n",
            "Save weights...\n",
            "Epoch 24/30.\tTraining loss : 1.6242\tValidation loss : 1.6486\n",
            "Validation loss decrease 1.6298 ----> 1.6280.\n",
            "Save weights...\n",
            "Epoch 25/30.\tTraining loss : 1.6073\tValidation loss : 1.6465\n",
            "Validation loss decrease 1.6280 ----> 1.6251.\n",
            "Save weights...\n",
            "Epoch 26/30.\tTraining loss : 1.5896\tValidation loss : 1.6380\n",
            "Validation loss decrease 1.6251 ----> 1.6207.\n",
            "Save weights...\n",
            "Epoch 27/30.\tTraining loss : 1.5596\tValidation loss : 1.6372\n",
            "Validation loss decrease 1.6207 ----> 1.6194.\n",
            "Save weights...\n",
            "Epoch 28/30.\tTraining loss : 1.5480\tValidation loss : 1.6400\n",
            "Validation loss decrease 1.6194 ----> 1.6194.\n",
            "Save weights...\n",
            "Epoch 29/30.\tTraining loss : 1.5291\tValidation loss : 1.6255\n",
            "Validation loss decrease 1.6194 ----> 1.6116.\n",
            "Save weights...\n",
            "Epoch 30/30.\tTraining loss : 1.5468\tValidation loss : 1.6287\n",
            "------------------------------------------------\n",
            "Model use embedding layer !!!\n",
            "Epoch 1/30.\tTraining loss : 2.6707\tValidation loss : 2.6596\n",
            "Validation loss decrease    inf ----> 2.6618.\n",
            "Save weights...\n",
            "Epoch 2/30.\tTraining loss : 2.2572\tValidation loss : 2.2110\n",
            "Validation loss decrease 2.6618 ----> 2.2139.\n",
            "Save weights...\n",
            "Epoch 3/30.\tTraining loss : 2.0645\tValidation loss : 2.0209\n",
            "Validation loss decrease 2.2139 ----> 2.0189.\n",
            "Save weights...\n",
            "Epoch 4/30.\tTraining loss : 1.9391\tValidation loss : 1.9112\n",
            "Validation loss decrease 2.0189 ----> 1.8977.\n",
            "Save weights...\n",
            "Epoch 5/30.\tTraining loss : 1.8505\tValidation loss : 1.8483\n",
            "Validation loss decrease 1.8977 ----> 1.8331.\n",
            "Save weights...\n",
            "Epoch 6/30.\tTraining loss : 1.8094\tValidation loss : 1.7977\n",
            "Validation loss decrease 1.8331 ----> 1.7792.\n",
            "Save weights...\n",
            "Epoch 7/30.\tTraining loss : 1.7709\tValidation loss : 1.7537\n",
            "Validation loss decrease 1.7792 ----> 1.7365.\n",
            "Save weights...\n",
            "Epoch 8/30.\tTraining loss : 1.7416\tValidation loss : 1.7264\n",
            "Validation loss decrease 1.7365 ----> 1.7059.\n",
            "Save weights...\n",
            "Epoch 9/30.\tTraining loss : 1.6827\tValidation loss : 1.6941\n",
            "Validation loss decrease 1.7059 ----> 1.6714.\n",
            "Save weights...\n",
            "Epoch 10/30.\tTraining loss : 1.6554\tValidation loss : 1.6711\n",
            "Validation loss decrease 1.6714 ----> 1.6436.\n",
            "Save weights...\n",
            "Epoch 11/30.\tTraining loss : 1.6218\tValidation loss : 1.6749\n",
            "Epoch 12/30.\tTraining loss : 1.6149\tValidation loss : 1.6383\n",
            "Validation loss decrease 1.6436 ----> 1.6087.\n",
            "Save weights...\n",
            "Epoch 13/30.\tTraining loss : 1.5714\tValidation loss : 1.6260\n",
            "Validation loss decrease 1.6087 ----> 1.5911.\n",
            "Save weights...\n",
            "Epoch 14/30.\tTraining loss : 1.5632\tValidation loss : 1.6209\n",
            "Validation loss decrease 1.5911 ----> 1.5848.\n",
            "Save weights...\n",
            "Epoch 15/30.\tTraining loss : 1.5504\tValidation loss : 1.6212\n",
            "Validation loss decrease 1.5848 ----> 1.5836.\n",
            "Save weights...\n",
            "Epoch 16/30.\tTraining loss : 1.5400\tValidation loss : 1.6086\n",
            "Validation loss decrease 1.5836 ----> 1.5718.\n",
            "Save weights...\n",
            "Epoch 17/30.\tTraining loss : 1.5240\tValidation loss : 1.6356\n",
            "Epoch 18/30.\tTraining loss : 1.5060\tValidation loss : 1.6007\n",
            "Validation loss decrease 1.5718 ----> 1.5609.\n",
            "Save weights...\n",
            "Epoch 19/30.\tTraining loss : 1.4787\tValidation loss : 1.6050\n",
            "Validation loss decrease 1.5609 ----> 1.5609.\n",
            "Save weights...\n",
            "Epoch 20/30.\tTraining loss : 1.4493\tValidation loss : 1.6118\n",
            "Epoch 21/30.\tTraining loss : 1.4159\tValidation loss : 1.6188\n",
            "Epoch 22/30.\tTraining loss : 1.3975\tValidation loss : 1.6230\n",
            "Epoch 23/30.\tTraining loss : 1.3878\tValidation loss : 1.6281\n",
            "Epoch 24/30.\tTraining loss : 1.3688\tValidation loss : 1.6327\n",
            "Epoch 25/30.\tTraining loss : 1.3220\tValidation loss : 1.6410\n",
            "Epoch 26/30.\tTraining loss : 1.3059\tValidation loss : 1.6628\n",
            "Epoch 27/30.\tTraining loss : 1.3001\tValidation loss : 1.6802\n",
            "Epoch 28/30.\tTraining loss : 1.2777\tValidation loss : 1.6813\n",
            "Epoch 29/30.\tTraining loss : 1.2465\tValidation loss : 1.6916\n",
            "Epoch 30/30.\tTraining loss : 1.1900\tValidation loss : 1.7061\n",
            "------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlN7jpnQ41e-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "def predict(net, char, use_one_hot, h=None, top_k=None):\n",
        "\n",
        "  x = np.array([[char2int[char]]])\n",
        "  if use_one_hot:\n",
        "    x = one_hot_encode(x, len(chars))\n",
        "  inputs = torch.from_numpy(x).to(device)\n",
        "  h = tuple([each.data for each in h])\n",
        "  out, h = net(inputs, h)\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  p = p.to(\"cpu\")\n",
        "\n",
        "  if top_k is None:\n",
        "    top_ch = np.arange(len(chars))\n",
        "  else:\n",
        "    p, top_ch = p.topk(top_k)\n",
        "    top_ch = top_ch.numpy().squeeze()\n",
        "  \n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "  return int2char[char], h "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZusEitgq45Mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, use_one_hot, prime='The', top_k=None):\n",
        "  net.to(device)\n",
        "  net.eval()\n",
        "  chars = [ch for ch in prime]\n",
        "  h = net.init_hidden(1)\n",
        "  for ch in prime:\n",
        "    char, h = predict(net, ch, use_one_hot, h, top_k=top_k)\n",
        "  \n",
        "  chars.append(char)\n",
        "\n",
        "  for ii in range(size):\n",
        "    char, h = predict(net, char[-1], use_one_hot, h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "  \n",
        "  return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oArMC0tZ7c_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b722fe1-1f9a-4282-a3e2-69adeabae26e"
      },
      "source": [
        "embedding_model.load_state_dict(torch.load('./model_0.h5'))\n",
        "print(sample(embedding_model, 1000, use_one_hot=False, prime='Trăm năm', top_k=5))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trăm năm ngọc nào đường\n",
            "\n",
            " Thiệt cho đã dễ ngọn ngày,\n",
            "\n",
            "Thôi cho ngọc thế tiếc thư lạ chàng\n",
            "\n",
            " Ngoại lời thế thấy thanh mây,\n",
            "\n",
            " Những nghiều nghiên thấy một lời vào này\n",
            "\n",
            "Nỗi ra thì chẳng ngang người,\n",
            "\n",
            " Đươm đường ngọn mới bạc vào chuyên trên\n",
            "\n",
            "Thương nhau nhắn nghĩa đến tay,\n",
            "\n",
            "Mày thương chẳng một người cho thế thông\n",
            "\n",
            "Nghĩ trời đá đã đã trong,\n",
            "\n",
            " Biết tình đài mới trước tay trước ngoài \n",
            "\n",
            "Tiếp đà thiếp chẳng mưa đường,\n",
            "\n",
            "Một đâu mà đã bán mày chẳng ngườ\n",
            "\n",
            " Bốn tay như nói trong dâo,\n",
            "\n",
            " Nàng điều cho chút như châu Thôi chăng:\n",
            "\n",
            "Nghe đưa đã bạc chưa ngươi,\n",
            "\n",
            "Người này những đã biết đầu thấy chinh?\n",
            "\n",
            " Nghĩ cha đã trước cho lời,\n",
            "\n",
            "Một trời nàng cũng chi ngày thư ra \n",
            "\n",
            " Đường thuyền lại nghĩ ngày ta,\n",
            "\n",
            "Nhìn đường thì cũng cho đâu chẳng người:\n",
            "\n",
            "Chiều rướm trăm đức đã đường,\n",
            "\n",
            " Thương càng cũng tiếng cho cho cho trường ?\n",
            "\n",
            "Buồn chinh lại đã đổn thuênh,\n",
            "\n",
            " Như rời thân đã cửa ngay thế thì!\n",
            "\n",
            "Thấp người thiểp một mặt may,\n",
            "\n",
            "Mà cho chẳng nguyệt đã chàng tinh hoa\n",
            "\n",
            "Thúy tinh thấp có đến thưa,\n",
            "\n",
            "Một ngay đã nghìn ngọn đàn nói lần\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usTtqvJsJ-Us",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c3a06654-9753-4e76-b376-c0fdb3ff327d"
      },
      "source": [
        "one_hot_model.load_state_dict(torch.load('./model_1.h5'))\n",
        "print(sample(one_hot_model, 1000, use_one_hot=True, prime='Trăm năm', top_k=5))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trăm năm\n",
            "\n",
            "Nghe rời được mặt ngoại trời,\n",
            "\n",
            "Một người, nhi nói thưa ra chẳng lờ\n",
            "\n",
            "Ngọn thư buồn ngận thế nào,\n",
            "\n",
            "Tho quân thuy cát mười vào cửa như?\n",
            "\n",
            "Thương đao thanh mới thì tha,\n",
            "\n",
            "Thưa cho đã chẳng chẳng là cũng nhâu\n",
            "\n",
            " Thôi trời đánh nhận, trăng chàng,\n",
            "\n",
            "Thương nghe trước đánh, mai trời có tay\n",
            "\n",
            " Đã lầu được người thanh đây,\n",
            "\n",
            " Thanh cho thong một nói đàu nhuy ta \n",
            "\n",
            "Chàng công nghe nỗi tin hoa,\n",
            "\n",
            " Nỗi lời, cũng ngắm một nha trong tì nên \n",
            "\n",
            "Biếc người lướm đã trăm người,\n",
            "\n",
            "Trong lòng chong chẳng cũng thai trong ngay\n",
            "\n",
            "Nghĩ triêm tan thúc thi hồng,\n",
            "\n",
            "Thanh cho đã bấy như thì đết đan !\n",
            "\n",
            "Đã thương thì nỗi những tiền,\n",
            "\n",
            "Chẳng cho có mách thiệng lời ngày ngươi \n",
            "\n",
            " Đau thanh đã chén cho thân,\n",
            "\n",
            " Còn cang nàng chiệm trưng trong thước trăg \n",
            "\n",
            "Chẳng như chẳng thút tinh càng,\n",
            "\n",
            " Ba đây cho đã ngọi canh đám triêng\n",
            "\n",
            "Người về trăm chẳng trăn cào,\n",
            "\n",
            " Thong đân mười má lại đồi nghe nơi\n",
            "\n",
            " Chàng cho người tiếc thanh thang,\n",
            "\n",
            "Nghĩ tin nhà cát mang đà lấn tam\n",
            "\n",
            "Thiều thay một ngọc thì thười,\n",
            "\n",
            "Một mâu nghĩ đáng mấy mình là người\n",
            "\n",
            "Từ cô \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cNfK8n9UZZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}